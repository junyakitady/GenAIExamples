{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ogaDN6kxNR"
      },
      "source": [
        "# PDF 文書の内容にもとづいて回答するボットの作り方を理解する\n",
        "## はじめに\n",
        "生成 AI や大規模言語モデル （LLM）は、膨大な量のテキストデータのトレーニングを受けており、テキストを生成し、言語を翻訳し、さまざまな種類のクリエイティブ コンテンツを作成し、有益な方法で質問に答えることができます。ただし、LLM には、企業で使用する場合の問題点がいくつかあります。\n",
        "その一つに、生成 AI は誤った情報を含むテキストを生成したり、誤った情報を翻訳したりする可能性があります。企業における生成 AI 利用では、これが問題となることがあります。\n",
        "\n",
        "そこで生成 AI をエンジンとして利用しながら、企業内の信頼のおける情報の中から適切な回答してほしいというユースケースがあります。\n",
        "\n",
        "これを実現する大きな処理の流れは次のようになっています。\n",
        "\n",
        "1.   PDF ファイルを読み込む\n",
        "2.   ページごとでもよいのですが、もう少し細かい単位にテキストを分割する\n",
        "3.   分割したテキスト情報をエンべディング API を利用してベクトル化 / エンべディング\n",
        "4.   ベクトル情報とテキスト情報の関連を保持する\n",
        "5.   ユーザーの問い合わせ内容をベクトル化 / エンべディングする\n",
        "6.   5. のベクトルと最も類似度の高いベクトルを複数個取得する\n",
        "7.   6. のベクトルを生成元のテキスト情報を取得する\n",
        "8.   複数個のテキスト情報をコンテキスト情報として与え、この情報の中から質問の回答を作成するように LLM に問い合わせをする\n",
        "9.   PDF 文書の内容にもとづく回答が得られます\n",
        "\n",
        "これを実現するために次の技術要素が必要となりますが、Google Cloud ではそれぞれに対応するソリューションを提供しています。\n",
        " - エンべディング --> Vertex AI Embeddings for Text\n",
        " - ベクトル検索 --> Vertex AI Vector Search ( AlloyDB pgvector / CloudSQL pgvector でも実現可能)\n",
        " - 最終回答の生成 --> Vertex AI Gemini API\n",
        "\n",
        "また、上記の処理フローをフルスクラッチで開発するとそれなりに工数が必要ですが、生成 AI 利用時のフレームワークである LangChain を利用すると簡単に実現できてしまいます。コーディング上は 3. - 4. で1行、5. - 8.が1行で済んでしまうのは開発者にとってはありがたいです。LangChain 様々です。\n",
        "\n",
        "この処理を、Google Cloud の Vertex AI を利用して確認します。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YZN7V8SNq4-s"
      },
      "source": [
        "## 環境セットアップ\n",
        "\n",
        "python のバージョンを確認します。最新の LangChain は `Requires-Python >=3.8.1,<4.0`が前提となっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svXE7ZWa_sZj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "opZUiJat_s7V"
      },
      "source": [
        "前提パッケージを導入します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkYAr6d4jNuA"
      },
      "outputs": [],
      "source": [
        "# Install Vertex AI LLM SDK\n",
        "! pip install langchain langchain-community langchain-text-splitters langchain-google-vertexai pypdf PyCryptodome chromadb --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**※ 注意: ここでカーネルを再起動します。**\n",
        "\n",
        "* Colab の場合、上記のログに\"RESTART RUNTIME\"ボタンをが表示された場合、ボタンを押してカーネルをリスタートできます。\n",
        "* Vertex AI Workbench の場合、メニューよりカーネルのリスタートを実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJPoHHSPlMQw"
      },
      "source": [
        "続いて、Google Cloud でプロジェクトを作成し Vertex AI API を有効化します。\n",
        "\n",
        "また、このコードを実行するユーザーに`Vertex AI ユーザー`のロールを付与します。\n",
        "\n",
        "Colab の場合、以下を実行し Vertex AI API のユーザー権限をもつアカウントでログインします。 Vertex AI Workbench の場合はスキップされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-IsvQA9mP62"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ytcQ_o2fC3Sm"
      },
      "source": [
        "環境変数などを定義します。 Google Cloud のプロジェクト ID を指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou_H3H5FmyLb"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"<your_project_id>\"  # @param {type:\"string\"}\n",
        "REGION = \"asia-northeast1\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dym2cLCHDPh_"
      },
      "source": [
        "Vertex AI と LangChain のライブラリーの導入を確認します。 LangChain v0.0.208 で動作確認しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CyuusrQoFVw"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "198MC9RDrcTP"
      },
      "source": [
        "## Vertex AI Gemini API の準備\n",
        "\n",
        "LangChain を利用して Gemini API のText、Chat、Embeddings モデルを取得します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XO9IEnyo25g"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "# Text model instance integrated with LangChain\n",
        "llm = VertexAI(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.5,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Chat instance integrated with LangChain\n",
        "chat = ChatVertexAI()\n",
        "\n",
        "# Embeddings API integrated with LangChain\n",
        "embedding = VertexAIEmbeddings(model_name=\"textembedding-gecko-multilingual@latest\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mux7FEUUs72C"
      },
      "source": [
        "## PDF の読み込みとベクトル化\n",
        "\n",
        "PDF ファイルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mk_FvW2xPdY"
      },
      "outputs": [],
      "source": [
        "# Ingest PDF files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# set PDF urls\n",
        "urls = [\n",
        "    # Google Cloud セキュリティ ホワイトペーパー\n",
        "    #\"https://services.google.com/fh/files/misc/security_whitepapers_4_booklet_jp.pdf\", \n",
        "    # 「Google Cloud Day: Digital '22 - 15 のトピックから学ぶ」🌟eBook\n",
        "    \"https://lp.cloudplatformonline.com/rs/808-GJW-314/images/Google_ebooks_all_0614.pdf\",\n",
        "]\n",
        "documents = []\n",
        "for url in urls:\n",
        "    documents += PyPDFLoader(url).load()\n",
        "\n",
        "print(f\"# of documents = {len(documents)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-hDs0_CkHZ3y"
      },
      "source": [
        "PDF から抽出した 1 ページごとのテキスト情報を一度統合し、少しのオーバーラップを含むより小さなチャンクに分割します。PDF 文書の 1 ページの内容が多くなかったり、ページごとに内容が分かれている場合、このステップは省略可能です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF1dWP8-xWw_"
      },
      "outputs": [],
      "source": [
        "# split the documents into chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "texts=text_splitter.split_text(content)\n",
        "print(f\"# of texts = {len(texts)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWSaKD3Yf-K"
      },
      "source": [
        "抽出したテキスト情報をエンベディングと呼ばれるベクトル情報として保管します。\n",
        "ここではベクトル情報の保存検索にインメモリで動作する軽量な Chroma を利用します。\n",
        "\n",
        "大規模なベクターストアとして、Google Cloud では [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview?hl=ja) の利用を推奨しています。Vertex AI Vector Search は、拡張性が高くレイテンシが低いベクトル類似性マッチング（近似最近傍探索）サービスを提供します。\n",
        "\n",
        "また、エンベディングに textembedding-gecko を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-rFggCiyQi0"
      },
      "outputs": [],
      "source": [
        "# Store docs in local vectorstore as index\n",
        "# it may take a while since API is rate limited\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "db = Chroma.from_texts(texts, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6LE-Bsw1OKO"
      },
      "outputs": [],
      "source": [
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h8WbmIRuaxia"
      },
      "source": [
        "## PDF の内容で Q/A をする\n",
        "\n",
        "ユーザーが質問した質問に対して、ベクトル検索で取得した類似のテキスト情報の中から回答を出すようにします。この仕組みを簡単に実現するフレームワークとして LangChain の RetrievalQA を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lqTUnaD1SUw"
      },
      "outputs": [],
      "source": [
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Gemini API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R-lNIJn5bh0r"
      },
      "source": [
        "ここで質問を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJC10UuF1m0W"
      },
      "outputs": [],
      "source": [
        "#query = input(\"Enter query:\")\n",
        "query = \"Cloud Spanner の特徴を教えて下さい。\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jE_8aSb_bseS"
      },
      "source": [
        "LLM に直接問い合わせをした場合は、一般の知識に基づいて回答します。その内容を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTgFcvD8brgO"
      },
      "outputs": [],
      "source": [
        "llm.invoke(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Niy8GIieG7u"
      },
      "source": [
        "PDF 情報からの回答はどのようになるでしょう？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUriTu6Zbkqd"
      },
      "outputs": [],
      "source": [
        "response = qa.invoke(query)\n",
        "response[\"result\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WLvzVhKD5LYV"
      },
      "source": [
        "回答内容を作成したソースを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTiKPdEikE1F"
      },
      "outputs": [],
      "source": [
        "response[\"source_documents\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc1KGL8510vo"
      },
      "source": [
        "どうして PDF の内容をもとに回答しているかを理解するには、上で利用した LangChain の RetrievalQA の `stuff` タイプのソースコードを参照するのが分かりやすいです。\n",
        "\n",
        "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/question_answering/stuff_prompt.py\n",
        "\n",
        "```\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "```\n",
        "```\n",
        "次のコンテキストをもとに、最後の質問に答えてください。もし答えが分からない場合は、分からないと答えてください。勝手に答えをでっち上げないでください。\n",
        "```\n",
        "\n",
        "上記のように、最終の回答はプロンプト エンジニアリングで、正確な情報ソースをプロンプトにコンテキスト情報として渡し、そのコンテキスト情報の範囲内で回答するように LLM に依頼しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain Expression Language\n",
        "ここでは、上記と同じ処理を LangChain Expression Language (LCEL) で実装した場合のコードを示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "template = \"\"\"次のコンテキスト情報を利用して、最後の質問に答えてください。回答は300字程度で回答してください。:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | RunnableParallel({\n",
        "      \"result\": prompt | llm,\n",
        "      \"source_documents\": itemgetter(\"context\"),\n",
        "    })\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以上、ありがとうございました。\n",
        "\n",
        "## 参考情報\n",
        "- [Getting Started with LangChain 🦜️🔗 + Vertex AI PaLM API](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/orchestration/langchain/intro_langchain_palm_api.ipynb)\n",
        "- [Question Answering with Large Documents using LangChain 🦜🔗](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain.ipynb)\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html)\n",
        "- [Overview of Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
